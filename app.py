import streamlit as st
import pandas as pd
import ee
import json
import tempfile
import geemap.foliumap as geemap
import datetime
import requests
from huggingface_hub import InferenceClient


# --- 1. AUTHENTICATE AND INITIALIZE EARTH ENGINE ---
try:
    service_account_info = st.secrets["google_earth_engine"]
    service_account_email = service_account_info["client_email"]

    with tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False) as f:
        json.dump(dict(service_account_info), f)
        temp_json_path = f.name

    credentials = ee.ServiceAccountCredentials(service_account_email, temp_json_path)
    ee.Initialize(credentials)
except Exception as e:
    st.error("Google Earth Engine authentication failed. Please ensure your GEE secrets are configured correctly in Streamlit.")
    st.stop()


# --- 2. APP CONFIGURATION AND SIDEBAR ---
st.set_page_config(layout="wide")
st.sidebar.title("K2-DesertGuard Controller")

# Region selection
region_coords = {
    "Abu Dhabi": [24.466667, 54.366667],
    "Dubai": [25.276987, 55.296249],
    "Sharjah": [25.34626, 55.42093],
    "Al Ain": [24.2075, 55.7447],
}
region_name = st.sidebar.selectbox("Select UAE Region", list(region_coords.keys()))
lat, lon = region_coords[region_name]
map_center = [lat, lon]
region_geometry = ee.Geometry.Point(lon, lat).buffer(20000)

st.sidebar.info("This dashboard uses real satellite data from Google Earth Engine. Select a region to update the analysis.")
st.sidebar.markdown("---")

# AI Model selection in sidebar
st.sidebar.markdown("### ü§ñ AI Model Selection")
model_choice = st.sidebar.selectbox(
    "Select Reasoning Model",
    options=[
        "Qwen2.5-32B (K2-Think base)",
        "Llama-3.3-70B",
        "Llama-3.1-70B", 
        "Try K2-Think variants"
    ],
    index=0,  # Default to Qwen2.5 (closest to K2-Think)
    help="K2-Think is based on Qwen2.5-32B. Using the base model provides similar reasoning capabilities."
)


# --- 3. EARTH ENGINE DATA ANALYSIS FUNCTIONS ---
@st.cache_data
def get_live_data(_geometry):
    soil_moisture_collection = ee.ImageCollection("NASA_USDA/HSL/SMAP10KM_soil_moisture")
    latest_soil_moisture_img = soil_moisture_collection.sort("system:time_start", False).first()
    
    sentinel_collection = ee.ImageCollection("COPERNICUS/S2_SR").filterBounds(_geometry)
    latest_sentinel_img = sentinel_collection.filter(ee.Filter.lt("CLOUDY_PIXEL_PERCENTAGE", 20)).sort("system:time_start", False).first()
    
    avg_ndvi = latest_sentinel_img.normalizedDifference(['B8', 'B4']).reduceRegion(
        reducer=ee.Reducer.mean(), geometry=_geometry, scale=30
    ).get('nd')
    
    avg_soil_moisture = latest_soil_moisture_img.select('ssm').reduceRegion(
        reducer=ee.Reducer.mean(), geometry=_geometry, scale=10000
    ).get('ssm')
    
    return avg_ndvi.getInfo(), avg_soil_moisture.getInfo()


@st.cache_data
def get_historical_ndvi(_geometry):
    """Get historical NDVI data - optimized to avoid timeout"""
    try:
        end_date = datetime.datetime.now()
        start_date = end_date - datetime.timedelta(days=365)
        
        collection = (ee.ImageCollection("COPERNICUS/S2_SR")
                     .filterBounds(_geometry)
                     .filterDate(start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'))
                     .filter(ee.Filter.lt("CLOUDY_PIXEL_PERCENTAGE", 20)))
        
        def clip_and_add_ndvi(image):
            return image.normalizedDifference(['B8', 'B4']).rename('NDVI').copyProperties(image, ['system:time_start'])
        
        ndvi_collection = collection.map(clip_and_add_ndvi)
        
        months = []
        ndvi_values = []
        
        for i in range(12):
            month_start = (end_date - datetime.timedelta(days=30*(12-i))).strftime('%Y-%m-%d')
            month_end = (end_date - datetime.timedelta(days=30*(11-i))).strftime('%Y-%m-%d')
            
            monthly_collection = ndvi_collection.filterDate(month_start, month_end)
            
            mean_ndvi = monthly_collection.mean().reduceRegion(
                reducer=ee.Reducer.mean(),
                geometry=_geometry,
                scale=1000,
                maxPixels=1e8
            ).get('NDVI')
            
            ndvi_val = mean_ndvi.getInfo()
            if ndvi_val is not None:
                months.append(pd.to_datetime(month_start))
                ndvi_values.append(ndvi_val)
        
        monthly_ndvi = pd.Series(ndvi_values, index=months)
        return monthly_ndvi
        
    except Exception as e:
        st.warning(f"Using estimated historical data (computation limit reached)")
        dates = pd.date_range(end=datetime.datetime.now(), periods=12, freq='M')
        import numpy as np
        baseline = avg_ndvi_value if 'avg_ndvi_value' in globals() else 0.15
        values = [baseline + np.random.uniform(-0.03, 0.03) for _ in range(12)]
        return pd.Series(values, index=dates)


# --- 4. AI REASONING WITH MODEL SELECTION ---
def k2_think_reasoning(ndvi_val, soil_val, region, model_choice):
    """AI Reasoning with selectable models"""
    
    prompt = f"""You are an environmental scientist analyzing desertification risk in the UAE.

Region: {region}
Current Environmental Data:
- Vegetation Index (NDVI): {ndvi_val:.4f} (range: -1 to 1, where >0.2 is healthy vegetation)
- Soil Moisture: {soil_val:.2f} mm (typical range: 5-30 mm)

Task: Provide step-by-step analysis:
1. Desertification risk level (High/Medium/Low)
2. Your reasoning process
3. Scientific explanation
4. Specific recommendations

Analysis:"""

    try:
        api_key = st.secrets.get("cerebras", {}).get("api_key", "")
        
        if not api_key:
            raise Exception("Cerebras API key not configured")
        
        # Map selection to model names
        if "Qwen2.5-32B" in model_choice:
            models = ["qwen2.5-32b", "qwen-2.5-32b"]
            display_name = "Qwen2.5-32B (K2-Think Base)"
        elif "Llama-3.3-70B" in model_choice:
            models = ["llama-3.3-70b", "llama3.3-70b"]
            display_name = "Llama 3.3 70B"
        elif "Llama-3.1-70B" in model_choice:
            models = ["llama-3.1-70b", "llama3.1-70b"]
            display_name = "Llama 3.1 70B"
        else:  # Try K2-Think variants
            models = ["llm360/k2-32b", "k2-think-32b", "k2-think", "LLM360/K2-Think", "llm360-k2-think", "cerebras/k2-think"]
            display_name = "K2-Think"
        
        last_error = None
        for model_name in models:
            try:
                response = requests.post(
                    "https://api.cerebras.ai/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": model_name,
                        "messages": [
                            {"role": "system", "content": "You are an expert environmental scientist specializing in desertification analysis."},
                            {"role": "user", "content": prompt}
                        ],
                        "max_tokens": 2048,
                        "temperature": 0.7
                    },
                    timeout=30
                )
                
                if response.status_code == 200:
                    result = response.json()
                    ai_response = result['choices'][0]['message']['content']
                    
                    # Parse risk
                    risk = "Medium"
                    if any(word in ai_response.lower() for word in ["high risk", "severe", "critical"]):
                        risk = "High"
                    elif any(word in ai_response.lower() for word in ["low risk", "stable", "minimal"]):
                        risk = "Low"
                    
                    trace = [
                        f"ü§ñ **{display_name} Analysis for {region}**",
                        f"üìä **Input Data:**",
                        f"  - NDVI: **{ndvi_val:.3f}**",
                        f"  - Soil Moisture: **{soil_val:.2f} mm**",
                        "",
                        "üß† **AI Reasoning:**",
                        ai_response
                    ]
                    
                    return risk, trace, ai_response
                else:
                    last_error = f"{model_name}: Status {response.status_code}"
                    continue
                    
            except Exception as e:
                last_error = f"{model_name}: {str(e)}"
                continue
        
        raise Exception(f"Model unavailable. Last: {last_error}")
            
    except Exception as e:
        st.warning(f"‚ö†Ô∏è AI model unavailable: {str(e)}")
        st.info("üìå Using rule-based environmental analysis")
        
        # Fallback
        trace = [
            f"Analyzing {region}...",
            f"NDVI: {ndvi_val:.3f}, Soil Moisture: {soil_val:.2f} mm"
        ]
        
        risk = "Low"
        if ndvi_val < 0.1 or soil_val < 10:
            risk = "High"
            trace.append("‚ö†Ô∏è Critical vegetation stress detected")
        elif ndvi_val < 0.15 or soil_val < 20:
            risk = "Medium"
            trace.append("‚ö†Ô∏è Moderate environmental stress")
        else:
            trace.append("‚úÖ Stable conditions")
        
        recommendations = {
            "High": "**Urgent interventions:** Water conservation, soil restoration, drought-resistant planting",
            "Medium": "**Preventive actions:** Increase monitoring, promote water efficiency, protect vegetation",
            "Low": "**Maintenance:** Continue sustainable practices, regular monitoring"
        }[risk]
        
        return risk, trace, recommendations


#When hugging face K2-think API will be provided
# def k2_think_reasoning(ndvi_val, soil_val, region):
#     """K2-Think AI Reasoning via Hugging Face Inference"""
    
#     prompt = f"""You are an environmental scientist analyzing desertification risk in the UAE.

# Region: {region}
# Current Environmental Data:
# - Vegetation Index (NDVI): {ndvi_val:.4f} (range: -1 to 1, where >0.2 is healthy vegetation)
# - Soil Moisture: {soil_val:.2f} mm (typical range: 5-30 mm)

# Task: Provide step-by-step analysis:
# 1. Desertification risk level (High/Medium/Low)
# 2. Your reasoning process
# 3. Scientific explanation
# 4. Specific recommendations

# Think step-by-step and show your reasoning:"""

#     try:
#         # Use Hugging Face Inference API
#         # Safe access to token
#         hf_token = st.secrets.get("huggingface", {}).get("token", "")
        
#         if not hf_token:
#             raise Exception("Hugging Face token not configured")
        
#         client = InferenceClient(token=hf_token)

#         # Generate response using text_generation
#         response = client.text_generation(
#             prompt,
#             model="LLM360/K2-Think",
#             max_new_tokens=2048,
#             temperature=0.7,
#             return_full_text=False,
#             stream=False
#         )
        
#         ai_response = response
        
#         # Parse risk level
#         risk = "Medium"
#         response_lower = ai_response.lower()
#         if any(word in response_lower for word in ["high risk", "severe", "critical"]):
#             risk = "High"
#         elif any(word in response_lower for word in ["low risk", "stable", "minimal"]):
#             risk = "Low"
        
#         # Format trace
#         trace = [
#             f"ü§ñ **K2-Think Analysis for {region}**",
#             f"üìä **Input Data:**",
#             f"  - NDVI: **{ndvi_val:.3f}**",
#             f"  - Soil Moisture: **{soil_val:.2f} mm**",
#             "",
#             "üß† **K2-Think Reasoning:**",
#             ai_response
#         ]
        
#         return risk, trace, ai_response
        
#     except Exception as e:
#         st.warning(f"‚ö†Ô∏è K2-Think unavailable: {str(e)}")
        
#         # Fallback to rule-based system
#         trace = [
#             f"Analyzing {region}...",
#             f"NDVI: {ndvi_val:.3f}, Soil Moisture: {soil_val:.2f} mm"
#         ]
        
#         risk = "Low"
#         if ndvi_val < 0.1 or soil_val < 10:
#             risk = "High"
#             trace.append("‚ö†Ô∏è Critical vegetation stress detected")
#         elif ndvi_val < 0.15 or soil_val < 20:
#             risk = "Medium"
#             trace.append("‚ö†Ô∏è Moderate environmental stress")
#         else:
#             trace.append("‚úÖ Stable conditions")
        
#         recommendations = {
#             "High": "**Urgent interventions:** Water conservation, soil restoration, drought-resistant planting",
#             "Medium": "**Preventive actions:** Increase monitoring, promote water efficiency, protect vegetation",
#             "Low": "**Maintenance:** Continue sustainable practices, regular monitoring"
#         }[risk]
        
#         return risk, trace, recommendations

# --- 5. MAIN APP LAYOUT AND VISUALIZATION ---
st.title(f"K2-DesertGuard: Live Environmental Monitor for {region_name}")
st.caption("Powered by Google Earth Engine & Real-Time Satellite Imagery")

# Fetch and process data
avg_ndvi_value, avg_soil_moisture_value = get_live_data(region_geometry)
risk_level, reasoning_trace, recommendations = k2_think_reasoning(avg_ndvi_value, avg_soil_moisture_value, region_name, model_choice)

# Display live metrics
col_a, col_b, col_c = st.columns(3)
col_a.metric("Desertification Risk", risk_level)
col_b.metric("Avg. Vegetation (NDVI)", f"{avg_ndvi_value:.3f}")
col_c.metric("Avg. Soil Moisture", f"{avg_soil_moisture_value:.2f} mm")

st.markdown("---")

# Map visualizations
col1, col2 = st.columns(2)

with col1:
    st.markdown("#### Live Vegetation Health (NDVI)")
    image = ee.ImageCollection("COPERNICUS/S2_SR").filterBounds(region_geometry).filter(ee.Filter.lt("CLOUDY_PIXEL_PERCENTAGE", 10)).sort("system:time_start", False).first()
    ndvi = image.normalizedDifference(['B8', 'B4'])
    ndvi_palette = ["#ff0000", "#ffff00", "#00ff00"]
    ndvi_vis_params = {"min": 0.0, "max": 0.5, "palette": ndvi_palette}
    
    Map_NDVI = geemap.Map(location=map_center, zoom_start=9)
    Map_NDVI.addLayer(ndvi, ndvi_vis_params, "Live NDVI")
    Map_NDVI.add_colormap(vmin=0.0, vmax=0.5, palette=ndvi_palette, label="Vegetation Density")
    Map_NDVI.to_streamlit(height=400)

with col2:
    st.markdown("#### Land Use & Land Cover")
    landcover = ee.ImageCollection("ESA/WorldCover/v100").first()
    Map_LULC = geemap.Map(location=map_center, zoom_start=9)
    Map_LULC.addLayer(landcover, {}, "Land Cover")
    Map_LULC.add_legend(title="ESA Land Cover", builtin_legend="ESA_WorldCover")
    Map_LULC.to_streamlit(height=400)

st.markdown("---")

# Historical chart and AI reasoning
col3, col4 = st.columns([2, 1])

with col3:
    st.markdown("#### Historical Vegetation Trend (12 Months)")
    historical_ndvi_data = get_historical_ndvi(region_geometry)
    st.line_chart(historical_ndvi_data)

with col4:
    st.markdown("#### AI Reasoning")
    for step in reasoning_trace:
        st.write(step)
    
    st.markdown("#### Recommendations")
    st.info(recommendations)

st.sidebar.info("""
**‚ÑπÔ∏è About Models:**
- **Qwen2.5-32B**: Base model for K2-Think with excellent reasoning
- **Llama-3.3-70B**: Larger model with strong capabilities
- **K2-Think variants**: Attempts direct K2-Think access

**Note:** Using Cerebras API as Hugging Face Inference API doesn't support K2-Think's 32B model size for free tier deployment.
""")